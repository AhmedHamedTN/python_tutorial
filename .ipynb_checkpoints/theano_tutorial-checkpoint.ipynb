{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Theano Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code is from Theano Tutorial: http://deeplearning.net/software/theano/tutorial/\n",
    "# See this tutorial too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import *\n",
    "import theano.tensor as T\n",
    "from theano import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "28.4\n"
     ]
    }
   ],
   "source": [
    "# A function to add to scalars\n",
    "x = T.dscalar('x')\n",
    "y = T.dscalar('y')\n",
    "z = x + y\n",
    "f = function([x, y], z)\n",
    "print f(2, 3)\n",
    "print f(16.3, 12.1)\n",
    "# T.dscalar is the type we assign to “0-dimensional arrays (scalar) of doubles (d)”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'theano.tensor.var.TensorVariable'>\n"
     ]
    }
   ],
   "source": [
    "# x and y are instances of TensorVariable. \n",
    "print type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, scalar)\n"
     ]
    }
   ],
   "source": [
    "# x and y are are assigned the theano Type dscalar in their type field:\n",
    "print x.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, scalar)\n"
     ]
    }
   ],
   "source": [
    "print z.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding two matrices:\n",
    "x = T.dmatrix('x')\n",
    "y = T.dmatrix('y')\n",
    "z = x + y\n",
    "f = function([x, y], z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'theano.tensor.var.TensorVariable'>\n"
     ]
    }
   ],
   "source": [
    "# Again x and y are instances of TensorVariable, but \n",
    "# dmatrix is the Type for matrices of doubles. \n",
    "print type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, matrix)\n"
     ]
    }
   ],
   "source": [
    "print x.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, matrix)\n"
     ]
    }
   ],
   "source": [
    "print z.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.  22.]\n",
      " [ 33.  44.]]\n"
     ]
    }
   ],
   "source": [
    "# Then we can use our new function on 2D arrays:. \n",
    "print f([[1, 2], [3, 4]], [[10, 20], [30, 40]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.,  22.],\n",
       "       [ 33.,  44.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The variable is a NumPy array. We can also use NumPy arrays directly as inputs:\n",
    "import numpy\n",
    "f(numpy.array([[1, 2], [3, 4]]), numpy.array([[10, 20], [30, 40]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plural Constructors\n",
    "from theano.tensor import *\n",
    "x, y, z = dmatrices(3) # creates three matrix Variables with no names\n",
    "x, y, z = dmatrices('x', 'y', 'z') # creates three matrix Variables named 'x', 'y' and 'z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorType(float64, matrix)\n"
     ]
    }
   ],
   "source": [
    "print x.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random numbers, etc.: http://deeplearning.net/software/theano/tutorial/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.device)\n",
    "print(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model:\n",
      "Final model:\n",
      "target values for D:\n",
      "[0 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 0 1\n",
      " 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0\n",
      " 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0\n",
      " 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 0\n",
      " 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1\n",
      " 0 0 1 1 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1\n",
      " 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0]\n",
      "prediction on D:\n",
      "[1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1\n",
      " 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1\n",
      " 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
      " 0 1 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 0 1\n",
      " 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0\n",
      " 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1\n",
      " 0 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1\n",
      " 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 0 0\n",
      " 0 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression: http://deeplearning.net/software/theano/tutorial/examples.html\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "\n",
    "N = 400                                   # training sample size\n",
    "feats = 784                               # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "D = (rng.rand(N, feats).astype(theano.config.floatX), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 100\n",
    "#np.asarray(your_data, dtype=theano.config.floatX)\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "\n",
    "# initialize the bias term\n",
    "b = theano.shared(0., name=\"b\")\n",
    "#print b.eval()\n",
    "print(\"Initial model:\")\n",
    "#print(w.get_value())\n",
    "#print(b.get_value())\n",
    "\n",
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
    "prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
    "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
    "                                          # w.r.t weight vector w and\n",
    "                                          # bias term b\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)),\n",
    "          allow_input_downcast=True) # added downcasting...\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "print(\"Final model:\")\n",
    "#print(w.get_value())\n",
    "#print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(D[1])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(D[0]))\n",
    "#----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "200\n",
      "200\n",
      "7142\n",
      "6994\n",
      "0\n",
      "200\n",
      "200\n",
      "0.0 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "200\n",
      "200\n",
      "(200, 7142)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 5-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.715\n",
      "********************\n",
      "precision_score\t0.765432098765\n",
      "recall_score\t0.62\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.81      0.74       100\n",
      "        1.0       0.77      0.62      0.69       100\n",
      "\n",
      "avg / total       0.72      0.71      0.71       200\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[81 19]\n",
      " [38 62]]\n"
     ]
    }
   ],
   "source": [
    "# Get text data\n",
    "#----------------\n",
    "from collections import namedtuple\n",
    "\n",
    "all_data = []  \n",
    "DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        label=line.split()[0]\n",
    "        word_list=line.lower().split()[1:]\n",
    "        all_data.append(DataDoc(label, word_list))\n",
    "        #print my_data[line_no]\n",
    "        #break\n",
    "train_data = all_data[:25000]\n",
    "test_data = all_data[25000:50000]\n",
    "print len(train_data)\n",
    "\n",
    "train_data=train_data[:100]+train_data[12500:12600]\n",
    "test_data=test_data[:100]+test_data[12500:12600]\n",
    "print len(train_data)\n",
    "print len(test_data)\n",
    "#--------------------\n",
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "from collections import defaultdict\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]=len(word_space)\n",
    "    return word_space\n",
    "\n",
    "word_space=get_space(train_data)\n",
    "print len(word_space)\n",
    "print word_space[\"love\"]\n",
    "#-------------------------\n",
    "import numpy as np\n",
    "\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    " \n",
    "\n",
    "train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "#test_vecs= get_sparse_vectors(test_data, word_space)\n",
    "\n",
    "#print train_vecs, test_vecs[0]\n",
    "print len(train_data[12500:12600])\n",
    "print len(train_vecs)\n",
    "print len(test_vecs)\n",
    "#-------------------------\n",
    "# We should usually get tags automatically based on input data file.\n",
    "# In the input data file we have, we know that the first 12500 data points are positive/1.0 and the next 12500 are\n",
    "# negative/0.0 then the next 12500 is poitive and the fourth chunk is negative.\n",
    "# So basically the train_data has 25K (with the first half positive and the second half negative)\n",
    "# and test_data with the same setup for class label. \n",
    "# The rest of the data in the file is unknown and we don't use that part.\n",
    "# We could write code to extract label automatically and we will do this based on a standardized format we will work with\n",
    "# later, for now we will hard-code the labels.\n",
    "\n",
    "from random import shuffle, randint\n",
    "\n",
    "\n",
    "train_tags=[ 1.0 for i in range(100)] + [ 0.0 for i in range(100)]\n",
    "test_tags=[ 1.0 for i in range(100)] + [ 0.0 for i in range(100)]\n",
    "\n",
    "\n",
    "#train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "#test_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "# Side note: If the first token in each line were the tag, we could get tags as follows:\n",
    "# tags= [train_data[i].tag for i in range(len(train_data))]\n",
    "print train_tags[-1], train_vecs[-1][:10]\n",
    "print len(train_tags)\n",
    "print len(test_tags)\n",
    "#--------------------\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "print train_vecs.shape\n",
    "#--------------------------------\n",
    "# Classification with scikit-learn\n",
    "# Now we have: train_tags, train_vecs, test_tags, test_vecs\n",
    "# Let's use sklearn to train an svm classifier:\n",
    "#-------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import gensim\n",
    "n_jobs = 2\n",
    "\n",
    "#train_vecs=array(train_vecs)\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "\n",
    "print type(train_tags)\n",
    "print type(train_vecs)\n",
    "clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "clf.fit(train_vecs, train_tags)\n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 5-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, train_vecs, train_tags, cv=5)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(train_tags, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(train_tags, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(train_tags, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(train_tags, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(train_tags, predicted)\n",
    "#----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 7142)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# This creates an artficial dataset (code from the Theano tutorial):\n",
    "D = (rng.rand(N, feats).astype(theano.config.floatX), rng.randint(size=N, low=0, high=2))\n",
    "#print D\n",
    "print D[0].shape\n",
    "print D[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.,  0.,  0., ...,  1.,  1.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  1.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  1.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  1.,  0.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  1.,  1.],\n",
      "       [ 0.,  0.,  0., ...,  1.,  1.,  1.]], dtype=float32), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "(200, 7142)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "#But let's use our data to construct D:\n",
    "# Let's ensure our x is float32, for use with Theano:\n",
    "x= train_vecs\n",
    "x=x.astype(theano.config.floatX)\n",
    "y=train_tags\n",
    "y=y.astype(int)\n",
    "# Now create the dataset, and check dimensions, etc.\n",
    "D=(x,y)\n",
    "print D\n",
    "print D[0].shape\n",
    "print D[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model:\n",
      "Final model:\n",
      "target values for D:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "prediction on D:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Use with logistic regression:\n",
    "# Logistic Regression: http://deeplearning.net/software/theano/tutorial/examples.html\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "#theano.config.optimizer='fast_compile'\n",
    "\n",
    "#N = 400                                   # training sample size\n",
    "#feats = 784# \n",
    "feats = 7142                              # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "#D = (rng.rand(N, feats).astype(theano.config.floatX), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 1000\n",
    "#np.asarray(your_data, dtype=theano.config.floatX)\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "\n",
    "# initialize the bias term\n",
    "b = theano.shared(0., name=\"b\")\n",
    "#print b.eval()\n",
    "print(\"Initial model:\")\n",
    "#print(w.get_value())\n",
    "#print(b.get_value())\n",
    "\n",
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
    "prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
    "gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
    "                                          # w.r.t weight vector w and\n",
    "                                          # bias term b\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)),\n",
    "          allow_input_downcast=True) # added downcasting...\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "print(\"Final model:\")\n",
    "#print(w.get_value())\n",
    "#print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(D[1])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(D[0]))\n",
    "#----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now try the code with different values of \"training_steps\" and see what you get.\n",
    "# For example, you can try:\n",
    "# training_steps= 100, training_steps=500, training_steps=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to create some functions to load the IBDB data:\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]=len(word_space)\n",
    "    return word_space\n",
    "\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    "def get_data():\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    all_data = []  \n",
    "    DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "    with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            label=line.split()[0]\n",
    "            word_list=line.lower().split()[1:]\n",
    "            all_data.append(DataDoc(label, word_list))\n",
    "    all_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "    all_tags+=all_tags\n",
    "    return all_data, all_tags\n",
    "    #--------------------------------------------------\n",
    "    \n",
    "all_data, all_tags= get_data()\n",
    "\n",
    "#train_data=train_data[:100]+train_data[12500:12600]\n",
    "#test_data=test_data[:100]+test_data[12500:12600]\n",
    "print len(train_data)\n",
    "print len(test_data)\n",
    "\n",
    "\n",
    "\n",
    "train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "\n",
    "\n",
    "train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]+ [ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "\n",
    "\n",
    "test_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "\n",
    "\n",
    "\n",
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "word_space=get_space(train_data)\n",
    "print len(word_space)\n",
    "print word_space[\"love\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_data = []  \n",
    "DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        label=line.split()[0]\n",
    "        word_list=line.lower().split()[1:]\n",
    "        all_data.append(DataDoc(label, word_list))\n",
    "        #print my_data[line_no]\n",
    "        #break\n",
    "train_data = all_data[:25000]\n",
    "test_data = all_data[25000:50000]\n",
    "print len(train_data)\n",
    "\n",
    "train_data=train_data[:100]+train_data[12500:12600]\n",
    "test_data=test_data[:100]+test_data[12500:12600]\n",
    "print len(train_data)\n",
    "print len(test_data)\n",
    "#--------------------\n",
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]=len(word_space)\n",
    "    return word_space\n",
    "\n",
    "word_space=get_space(train_data)\n",
    "print len(word_space)\n",
    "print word_space[\"love\"]\n",
    "#-------------------------\n",
    "import numpy as np\n",
    "\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    " \n",
    "\n",
    "train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "#test_vecs= get_sparse_vectors(test_data, word_space)\n",
    "\n",
    "#print train_vecs, test_vecs[0]\n",
    "print len(train_data[12500:12600])\n",
    "print len(train_vecs)\n",
    "print len(test_vecs)\n",
    "#-------------------------\n",
    "# We should usually get tags automatically based on input data file.\n",
    "# In the input data file we have, we know that the first 12500 data points are positive/1.0 and the next 12500 are\n",
    "# negative/0.0 then the next 12500 is poitive and the fourth chunk is negative.\n",
    "# So basically the train_data has 25K (with the first half positive and the second half negative)\n",
    "# and test_data with the same setup for class label. \n",
    "# The rest of the data in the file is unknown and we don't use that part.\n",
    "# We could write code to extract label automatically and we will do this based on a standardized format we will work with\n",
    "# later, for now we will hard-code the labels.\n",
    "\n",
    "from random import shuffle, randint\n",
    "\n",
    "\n",
    "train_tags=[ 1.0 for i in range(100)] + [ 0.0 for i in range(100)]\n",
    "test_tags=[ 1.0 for i in range(100)] + [ 0.0 for i in range(100)]\n",
    "\n",
    "\n",
    "#train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "#test_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "# Side note: If the first token in each line were the tag, we could get tags as follows:\n",
    "# tags= [train_data[i].tag for i in range(len(train_data))]\n",
    "print train_tags[-1], train_vecs[-1][:10]\n",
    "print len(train_tags)\n",
    "print len(test_tags)\n",
    "#--------------------\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "print train_vecs.shape\n",
    "#--------------------------------\n",
    "# Classification with scikit-learn\n",
    "# Now we have: train_tags, train_vecs, test_tags, test_vecs\n",
    "# Let's use sklearn to train an svm classifier:\n",
    "#-------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import gensim\n",
    "n_jobs = 2\n",
    "\n",
    "#train_vecs=array(train_vecs)\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "\n",
    "print type(train_tags)\n",
    "print type(train_vecs)\n",
    "clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "clf.fit(train_vecs, train_tags)\n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 5-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, train_vecs, train_tags, cv=5)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(train_tags, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(train_tags, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(train_tags, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(train_tags, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(train_tags, predicted)\n",
    "#----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space_len: 975\n",
      "train_vecs.shape: 25000, 975\n",
      "dev_vecs.shape: 5000, 975\n",
      "test_vecs.shape: 20000, 975\n",
      "Subtensor{int64}.0\n",
      "... building the model\n",
      "... training the model\n",
      "epoch 1, minibatch 41/41, validation error 52.083333 %\n",
      "     epoch 1, minibatch 41/41, test error of best model 50.505051 %\n",
      "epoch 2, minibatch 41/41, validation error 52.083333 %\n",
      "epoch 3, minibatch 41/41, validation error 52.083333 %\n",
      "epoch 4, minibatch 41/41, validation error 52.083333 %\n",
      "epoch 5, minibatch 41/41, validation error 52.083333 %\n",
      "epoch 6, minibatch 41/41, validation error 52.062500 %\n",
      "     epoch 6, minibatch 41/41, test error of best model 50.484848 %\n",
      "epoch 7, minibatch 41/41, validation error 51.895833 %\n",
      "     epoch 7, minibatch 41/41, test error of best model 50.328283 %\n",
      "epoch 8, minibatch 41/41, validation error 51.687500 %\n",
      "     epoch 8, minibatch 41/41, test error of best model 50.111111 %\n",
      "epoch 9, minibatch 41/41, validation error 51.354167 %\n",
      "     epoch 9, minibatch 41/41, test error of best model 49.944444 %\n",
      "epoch 10, minibatch 41/41, validation error 51.020833 %\n",
      "     epoch 10, minibatch 41/41, test error of best model 49.641414 %\n",
      "epoch 11, minibatch 41/41, validation error 50.500000 %\n",
      "     epoch 11, minibatch 41/41, test error of best model 49.363636 %\n",
      "epoch 12, minibatch 41/41, validation error 50.125000 %\n",
      "     epoch 12, minibatch 41/41, test error of best model 49.000000 %\n",
      "epoch 13, minibatch 41/41, validation error 49.687500 %\n",
      "     epoch 13, minibatch 41/41, test error of best model 48.661616 %\n",
      "epoch 14, minibatch 41/41, validation error 49.125000 %\n",
      "     epoch 14, minibatch 41/41, test error of best model 48.257576 %\n",
      "epoch 15, minibatch 41/41, validation error 48.645833 %\n",
      "     epoch 15, minibatch 41/41, test error of best model 47.853535 %\n",
      "epoch 16, minibatch 41/41, validation error 48.291667 %\n",
      "     epoch 16, minibatch 41/41, test error of best model 47.585859 %\n",
      "epoch 17, minibatch 41/41, validation error 47.979167 %\n",
      "     epoch 17, minibatch 41/41, test error of best model 47.217172 %\n",
      "epoch 18, minibatch 41/41, validation error 47.812500 %\n",
      "     epoch 18, minibatch 41/41, test error of best model 47.000000 %\n",
      "epoch 19, minibatch 41/41, validation error 47.604167 %\n",
      "     epoch 19, minibatch 41/41, test error of best model 46.737374 %\n",
      "epoch 20, minibatch 41/41, validation error 47.395833 %\n",
      "     epoch 20, minibatch 41/41, test error of best model 46.520202 %\n",
      "epoch 21, minibatch 41/41, validation error 47.041667 %\n",
      "     epoch 21, minibatch 41/41, test error of best model 46.292929 %\n",
      "epoch 22, minibatch 41/41, validation error 46.770833 %\n",
      "     epoch 22, minibatch 41/41, test error of best model 46.030303 %\n",
      "epoch 23, minibatch 41/41, validation error 46.645833 %\n",
      "     epoch 23, minibatch 41/41, test error of best model 45.813131 %\n",
      "epoch 24, minibatch 41/41, validation error 46.125000 %\n",
      "     epoch 24, minibatch 41/41, test error of best model 45.570707 %\n",
      "epoch 25, minibatch 41/41, validation error 45.895833 %\n",
      "     epoch 25, minibatch 41/41, test error of best model 45.348485 %\n",
      "epoch 26, minibatch 41/41, validation error 45.812500 %\n",
      "     epoch 26, minibatch 41/41, test error of best model 45.121212 %\n",
      "epoch 27, minibatch 41/41, validation error 45.666667 %\n",
      "     epoch 27, minibatch 41/41, test error of best model 44.994949 %\n",
      "epoch 28, minibatch 41/41, validation error 45.458333 %\n",
      "     epoch 28, minibatch 41/41, test error of best model 44.848485 %\n",
      "epoch 29, minibatch 41/41, validation error 45.312500 %\n",
      "     epoch 29, minibatch 41/41, test error of best model 44.676768 %\n",
      "epoch 30, minibatch 41/41, validation error 45.229167 %\n",
      "     epoch 30, minibatch 41/41, test error of best model 44.606061 %\n",
      "epoch 31, minibatch 41/41, validation error 45.125000 %\n",
      "     epoch 31, minibatch 41/41, test error of best model 44.515152 %\n",
      "epoch 32, minibatch 41/41, validation error 44.895833 %\n",
      "     epoch 32, minibatch 41/41, test error of best model 44.378788 %\n",
      "epoch 33, minibatch 41/41, validation error 44.833333 %\n",
      "     epoch 33, minibatch 41/41, test error of best model 44.303030 %\n",
      "epoch 34, minibatch 41/41, validation error 44.750000 %\n",
      "     epoch 34, minibatch 41/41, test error of best model 44.176768 %\n",
      "epoch 35, minibatch 41/41, validation error 44.500000 %\n",
      "     epoch 35, minibatch 41/41, test error of best model 44.080808 %\n",
      "epoch 36, minibatch 41/41, validation error 44.458333 %\n",
      "     epoch 36, minibatch 41/41, test error of best model 43.959596 %\n",
      "epoch 37, minibatch 41/41, validation error 44.416667 %\n",
      "     epoch 37, minibatch 41/41, test error of best model 43.878788 %\n",
      "epoch 38, minibatch 41/41, validation error 44.375000 %\n",
      "     epoch 38, minibatch 41/41, test error of best model 43.813131 %\n",
      "epoch 39, minibatch 41/41, validation error 44.354167 %\n",
      "     epoch 39, minibatch 41/41, test error of best model 43.757576 %\n",
      "epoch 40, minibatch 41/41, validation error 44.291667 %\n",
      "     epoch 40, minibatch 41/41, test error of best model 43.656566 %\n",
      "epoch 41, minibatch 41/41, validation error 44.250000 %\n",
      "     epoch 41, minibatch 41/41, test error of best model 43.530303 %\n",
      "epoch 42, minibatch 41/41, validation error 44.166667 %\n",
      "     epoch 42, minibatch 41/41, test error of best model 43.500000 %\n",
      "epoch 43, minibatch 41/41, validation error 44.166667 %\n",
      "epoch 44, minibatch 41/41, validation error 44.041667 %\n",
      "     epoch 44, minibatch 41/41, test error of best model 43.368687 %\n",
      "epoch 45, minibatch 41/41, validation error 44.104167 %\n",
      "epoch 46, minibatch 41/41, validation error 44.041667 %\n",
      "epoch 47, minibatch 41/41, validation error 44.041667 %\n",
      "epoch 48, minibatch 41/41, validation error 43.979167 %\n",
      "     epoch 48, minibatch 41/41, test error of best model 43.126263 %\n",
      "epoch 49, minibatch 41/41, validation error 43.979167 %\n",
      "epoch 50, minibatch 41/41, validation error 43.937500 %\n",
      "     epoch 50, minibatch 41/41, test error of best model 43.035354 %\n",
      "epoch 51, minibatch 41/41, validation error 43.875000 %\n",
      "     epoch 51, minibatch 41/41, test error of best model 43.015152 %\n",
      "epoch 52, minibatch 41/41, validation error 43.833333 %\n",
      "     epoch 52, minibatch 41/41, test error of best model 42.984848 %\n",
      "epoch 53, minibatch 41/41, validation error 43.708333 %\n",
      "     epoch 53, minibatch 41/41, test error of best model 42.868687 %\n",
      "epoch 54, minibatch 41/41, validation error 43.729167 %\n",
      "epoch 55, minibatch 41/41, validation error 43.708333 %\n",
      "epoch 56, minibatch 41/41, validation error 43.645833 %\n",
      "     epoch 56, minibatch 41/41, test error of best model 42.772727 %\n",
      "epoch 57, minibatch 41/41, validation error 43.541667 %\n",
      "     epoch 57, minibatch 41/41, test error of best model 42.727273 %\n",
      "epoch 58, minibatch 41/41, validation error 43.541667 %\n",
      "epoch 59, minibatch 41/41, validation error 43.520833 %\n",
      "     epoch 59, minibatch 41/41, test error of best model 42.691919 %\n",
      "epoch 60, minibatch 41/41, validation error 43.520833 %\n",
      "epoch 61, minibatch 41/41, validation error 43.500000 %\n",
      "     epoch 61, minibatch 41/41, test error of best model 42.616162 %\n",
      "epoch 62, minibatch 41/41, validation error 43.520833 %\n",
      "epoch 63, minibatch 41/41, validation error 43.500000 %\n",
      "epoch 64, minibatch 41/41, validation error 43.520833 %\n",
      "epoch 65, minibatch 41/41, validation error 43.437500 %\n",
      "     epoch 65, minibatch 41/41, test error of best model 42.515152 %\n",
      "epoch 66, minibatch 41/41, validation error 43.437500 %\n",
      "epoch 67, minibatch 41/41, validation error 43.416667 %\n",
      "     epoch 67, minibatch 41/41, test error of best model 42.474747 %\n",
      "epoch 68, minibatch 41/41, validation error 43.395833 %\n",
      "     epoch 68, minibatch 41/41, test error of best model 42.429293 %\n",
      "epoch 69, minibatch 41/41, validation error 43.395833 %\n",
      "epoch 70, minibatch 41/41, validation error 43.375000 %\n",
      "     epoch 70, minibatch 41/41, test error of best model 42.373737 %\n",
      "epoch 71, minibatch 41/41, validation error 43.354167 %\n",
      "     epoch 71, minibatch 41/41, test error of best model 42.368687 %\n",
      "epoch 72, minibatch 41/41, validation error 43.312500 %\n",
      "     epoch 72, minibatch 41/41, test error of best model 42.328283 %\n",
      "epoch 73, minibatch 41/41, validation error 43.270833 %\n",
      "     epoch 73, minibatch 41/41, test error of best model 42.313131 %\n",
      "epoch 74, minibatch 41/41, validation error 43.229167 %\n",
      "     epoch 74, minibatch 41/41, test error of best model 42.282828 %\n",
      "epoch 75, minibatch 41/41, validation error 43.229167 %\n",
      "epoch 76, minibatch 41/41, validation error 43.229167 %\n",
      "epoch 77, minibatch 41/41, validation error 43.208333 %\n",
      "     epoch 77, minibatch 41/41, test error of best model 42.202020 %\n",
      "epoch 78, minibatch 41/41, validation error 43.187500 %\n",
      "     epoch 78, minibatch 41/41, test error of best model 42.186869 %\n",
      "epoch 79, minibatch 41/41, validation error 43.166667 %\n",
      "     epoch 79, minibatch 41/41, test error of best model 42.166667 %\n",
      "epoch 80, minibatch 41/41, validation error 43.166667 %\n",
      "epoch 81, minibatch 41/41, validation error 43.145833 %\n",
      "     epoch 81, minibatch 41/41, test error of best model 42.151515 %\n",
      "epoch 82, minibatch 41/41, validation error 43.125000 %\n",
      "     epoch 82, minibatch 41/41, test error of best model 42.146465 %\n",
      "epoch 83, minibatch 41/41, validation error 43.083333 %\n",
      "     epoch 83, minibatch 41/41, test error of best model 42.126263 %\n",
      "epoch 84, minibatch 41/41, validation error 43.020833 %\n",
      "     epoch 84, minibatch 41/41, test error of best model 42.116162 %\n",
      "epoch 85, minibatch 41/41, validation error 43.020833 %\n",
      "epoch 86, minibatch 41/41, validation error 43.020833 %\n",
      "epoch 87, minibatch 41/41, validation error 42.979167 %\n",
      "     epoch 87, minibatch 41/41, test error of best model 42.070707 %\n",
      "epoch 88, minibatch 41/41, validation error 42.958333 %\n",
      "     epoch 88, minibatch 41/41, test error of best model 42.045455 %\n",
      "epoch 89, minibatch 41/41, validation error 42.937500 %\n",
      "     epoch 89, minibatch 41/41, test error of best model 42.030303 %\n",
      "epoch 90, minibatch 41/41, validation error 42.916667 %\n",
      "     epoch 90, minibatch 41/41, test error of best model 42.035354 %\n",
      "epoch 91, minibatch 41/41, validation error 42.895833 %\n",
      "     epoch 91, minibatch 41/41, test error of best model 42.030303 %\n",
      "epoch 92, minibatch 41/41, validation error 42.875000 %\n",
      "     epoch 92, minibatch 41/41, test error of best model 42.030303 %\n",
      "epoch 93, minibatch 41/41, validation error 42.875000 %\n",
      "epoch 94, minibatch 41/41, validation error 42.854167 %\n",
      "     epoch 94, minibatch 41/41, test error of best model 42.010101 %\n",
      "epoch 95, minibatch 41/41, validation error 42.854167 %\n",
      "epoch 96, minibatch 41/41, validation error 42.833333 %\n",
      "     epoch 96, minibatch 41/41, test error of best model 42.005051 %\n",
      "epoch 97, minibatch 41/41, validation error 42.791667 %\n",
      "     epoch 97, minibatch 41/41, test error of best model 41.984848 %\n",
      "epoch 98, minibatch 41/41, validation error 42.770833 %\n",
      "     epoch 98, minibatch 41/41, test error of best model 41.979798 %\n",
      "epoch 99, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 100, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 101, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 102, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 103, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 104, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 105, minibatch 41/41, validation error 42.791667 %\n",
      "epoch 106, minibatch 41/41, validation error 42.791667 %\n",
      "epoch 107, minibatch 41/41, validation error 42.791667 %\n",
      "epoch 108, minibatch 41/41, validation error 42.791667 %\n",
      "epoch 109, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 110, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 111, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 112, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 113, minibatch 41/41, validation error 42.770833 %\n",
      "epoch 114, minibatch 41/41, validation error 42.750000 %\n",
      "     epoch 114, minibatch 41/41, test error of best model 41.868687 %\n",
      "epoch 115, minibatch 41/41, validation error 42.750000 %\n",
      "epoch 116, minibatch 41/41, validation error 42.729167 %\n",
      "     epoch 116, minibatch 41/41, test error of best model 41.863636 %\n",
      "epoch 117, minibatch 41/41, validation error 42.750000 %\n",
      "epoch 118, minibatch 41/41, validation error 42.750000 %\n",
      "epoch 119, minibatch 41/41, validation error 42.750000 %\n",
      "epoch 120, minibatch 41/41, validation error 42.729167 %\n",
      "     epoch 120, minibatch 41/41, test error of best model 41.853535 %\n",
      "epoch 121, minibatch 41/41, validation error 42.729167 %\n",
      "epoch 122, minibatch 41/41, validation error 42.729167 %\n",
      "epoch 123, minibatch 41/41, validation error 42.708333 %\n",
      "     epoch 123, minibatch 41/41, test error of best model 41.833333 %\n",
      "epoch 124, minibatch 41/41, validation error 42.687500 %\n",
      "     epoch 124, minibatch 41/41, test error of best model 41.828283 %\n",
      "epoch 125, minibatch 41/41, validation error 42.666667 %\n",
      "     epoch 125, minibatch 41/41, test error of best model 41.818182 %\n",
      "epoch 126, minibatch 41/41, validation error 42.645833 %\n",
      "     epoch 126, minibatch 41/41, test error of best model 41.808081 %\n",
      "epoch 127, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 128, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 129, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 130, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 131, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 132, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 133, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 134, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 135, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 136, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 137, minibatch 41/41, validation error 42.645833 %\n",
      "epoch 138, minibatch 41/41, validation error 42.625000 %\n",
      "     epoch 138, minibatch 41/41, test error of best model 41.792929 %\n",
      "epoch 139, minibatch 41/41, validation error 42.604167 %\n",
      "     epoch 139, minibatch 41/41, test error of best model 41.792929 %\n",
      "epoch 140, minibatch 41/41, validation error 42.604167 %\n",
      "epoch 141, minibatch 41/41, validation error 42.604167 %\n",
      "epoch 142, minibatch 41/41, validation error 42.583333 %\n",
      "     epoch 142, minibatch 41/41, test error of best model 41.792929 %\n",
      "epoch 143, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 144, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 145, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 146, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 147, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 148, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 149, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 150, minibatch 41/41, validation error 42.583333 %\n",
      "epoch 151, minibatch 41/41, validation error 42.562500 %\n",
      "     epoch 151, minibatch 41/41, test error of best model 41.762626 %\n",
      "epoch 152, minibatch 41/41, validation error 42.562500 %\n",
      "epoch 153, minibatch 41/41, validation error 42.541667 %\n",
      "     epoch 153, minibatch 41/41, test error of best model 41.762626 %\n",
      "epoch 154, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 155, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 156, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 157, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 158, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 159, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 160, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 161, minibatch 41/41, validation error 42.541667 %\n",
      "epoch 162, minibatch 41/41, validation error 42.520833 %\n",
      "     epoch 162, minibatch 41/41, test error of best model 41.747475 %\n",
      "epoch 163, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 164, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 165, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 166, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 167, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 168, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 169, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 170, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 171, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 172, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 173, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 174, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 175, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 176, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 177, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 178, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 179, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 180, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 181, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 182, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 183, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 184, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 185, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 186, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 187, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 188, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 189, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 190, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 191, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 192, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 193, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 194, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 195, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 196, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 197, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 198, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 199, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 200, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 201, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 202, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 203, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 204, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 205, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 206, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 207, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 208, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 209, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 210, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 211, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 212, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 213, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 214, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 215, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 216, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 217, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 218, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 219, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 220, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 221, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 222, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 223, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 224, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 225, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 226, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 227, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 228, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 229, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 230, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 231, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 232, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 233, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 234, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 235, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 236, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 237, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 238, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 239, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 240, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 241, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 242, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 243, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 244, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 245, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 246, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 247, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 248, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 249, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 250, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 251, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 252, minibatch 41/41, validation error 42.520833 %\n",
      "epoch 253, minibatch 41/41, validation error 42.500000 %\n",
      "     epoch 253, minibatch 41/41, test error of best model 41.712121 %\n",
      "epoch 254, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 255, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 256, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 257, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 258, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 259, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 260, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 261, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 262, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 263, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 264, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 265, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 266, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 267, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 268, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 269, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 270, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 271, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 272, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 273, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 274, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 275, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 276, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 277, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 278, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 279, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 280, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 281, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 282, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 283, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 284, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 285, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 286, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 287, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 288, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 289, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 290, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 291, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 292, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 293, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 294, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 295, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 296, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 297, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 298, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 299, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 300, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 301, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 302, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 303, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 304, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 305, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 306, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 307, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 308, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 309, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 310, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 311, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 312, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 313, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 314, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 315, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 316, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 317, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 318, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 319, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 320, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 321, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 322, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 323, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 324, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 325, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 326, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 327, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 328, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 329, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 330, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 331, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 332, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 333, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 334, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 335, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 336, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 337, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 338, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 339, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 340, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 341, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 342, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 343, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 344, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 345, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 346, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 347, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 348, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 349, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 350, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 351, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 352, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 353, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 354, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 355, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 356, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 357, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 358, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 359, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 360, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 361, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 362, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 363, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 364, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 365, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 366, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 367, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 368, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 369, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 370, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 371, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 372, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 373, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 374, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 375, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 376, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 377, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 378, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 379, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 380, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 381, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 382, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 383, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 384, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 385, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 386, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 387, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 388, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 389, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 390, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 391, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 392, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 393, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 394, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 395, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 396, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 397, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 398, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 399, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 400, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 401, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 402, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 403, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 404, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 405, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 406, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 407, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 408, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 409, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 410, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 411, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 412, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 413, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 414, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 415, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 416, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 417, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 418, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 419, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 420, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 421, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 422, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 423, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 424, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 425, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 426, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 427, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 428, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 429, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 430, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 431, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 432, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 433, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 434, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 435, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 436, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 437, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 438, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 439, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 440, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 441, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 442, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 443, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 444, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 445, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 446, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 447, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 448, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 449, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 450, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 451, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 452, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 453, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 454, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 455, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 456, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 457, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 458, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 459, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 460, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 461, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 462, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 463, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 464, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 465, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 466, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 467, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 468, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 469, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 470, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 471, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 472, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 473, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 474, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 475, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 476, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 477, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 478, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 479, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 480, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 481, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 482, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 483, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 484, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 485, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 486, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 487, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 488, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 489, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 490, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 491, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 492, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 493, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 494, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 495, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 496, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 497, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 498, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 499, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 500, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 501, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 502, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 503, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 504, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 505, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 506, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 507, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 508, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 509, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 510, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 511, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 512, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 513, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 514, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 515, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 516, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 517, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 518, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 519, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 520, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 521, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 522, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 523, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 524, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 525, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 526, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 527, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 528, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 529, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 530, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 531, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 532, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 533, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 534, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 535, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 536, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 537, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 538, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 539, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 540, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 541, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 542, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 543, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 544, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 545, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 546, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 547, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 548, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 549, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 550, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 551, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 552, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 553, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 554, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 555, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 556, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 557, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 558, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 559, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 560, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 561, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 562, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 563, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 564, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 565, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 566, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 567, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 568, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 569, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 570, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 571, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 572, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 573, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 574, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 575, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 576, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 577, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 578, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 579, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 580, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 581, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 582, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 583, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 584, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 585, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 586, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 587, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 588, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 589, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 590, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 591, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 592, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 593, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 594, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 595, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 596, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 597, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 598, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 599, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 600, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 601, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 602, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 603, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 604, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 605, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 606, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 607, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 608, minibatch 41/41, validation error 42.500000 %\n",
      "epoch 609, minibatch 41/41, validation error 42.500000 %\n",
      "Optimization complete with best validation score of 42.500000 %,with test performance 41.712121 %\n",
      "The code run for 610 epochs, with 11.447371 epochs/sec\n",
      "The code for file best_model.pkl ran for 53.3s\n",
      "Now predicting...\n",
      "Predicted values for the first 10 examples in test set:\n",
      "[0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This tutorial introduces logistic regression using Theano and stochastic\n",
    "gradient descent.\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\"\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import shuffle, randint\n",
    "#----------------------------------------------------\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "#----------------------------------------------------\n",
    "\n",
    "def get_data():\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    all_data = []  \n",
    "    DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "    with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            label=line.split()[0]\n",
    "            word_list=line.lower().split()[1:]\n",
    "            all_data.append(DataDoc(label, word_list))\n",
    "    train_data = all_data[:25000]\n",
    "    dev_data = all_data[25000:27500]+all_data[47500:50000]\n",
    "    test_data=all_data[27500:47500]\n",
    "    # labels\n",
    "    train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "    dev_tags= [ 1.0 for i in range(2500)] + [ 0.0 for i in range(2500)]\n",
    "    test_tags= [ 1.0 for i in range(10000)] + [ 0.0 for i in range(10000)]\n",
    "    return train_data, train_tags, dev_data, dev_tags, test_data, test_tags\n",
    "    #--------------------------------------------------\n",
    "#train_data, train_tags, dev_data, dev_tags, test_data, test_tags=get_data()\n",
    "########################\n",
    "\n",
    "\n",
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]+=1\n",
    "    return word_space\n",
    "\n",
    "# train_data, train_tags, dev_data, dev_tags, test_data, test_tags=get_data()\n",
    "# word_space=get_space(train_data)\n",
    "# word_space={w: word_space[w] for w in word_space if word_space[w] > 500}\n",
    "# space_len=len(word_space)\n",
    "# print \"space_len: \", space_len\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    "   \n",
    "# train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "# test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "# dev_vecs= [get_sparse_vec(data_point, word_space) for data_point in dev_data]\n",
    "# #---------------------------\n",
    "# train_vecs=np.array(train_vecs)\n",
    "# train_tags=np.array(train_tags)\n",
    "# dev_vecs=np.array(dev_vecs)\n",
    "# dev_tags=np.array(dev_tags)\n",
    "# test_vecs=np.array(test_vecs)\n",
    "# test_tags=np.array(test_tags)\n",
    "# print train_vecs.shape\n",
    "# print dev_vecs.shape\n",
    "# print test_vecs.shape\n",
    "\n",
    "\n",
    "def load_data(train_vecs, train_tags, dev_vecs, dev_tags, test_vecs, test_tags):\n",
    "    #------------------------------\n",
    "    # Modified from Theano tutorial.\n",
    "    # I basically pass data_x, data_y instead of data_xy\n",
    "    def shared_dataset(data_x, data_y, borrow=True):\n",
    "        \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "        \"\"\"\n",
    "        shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                               dtype=theano.config.floatX), borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                               dtype=theano.config.floatX),\n",
    "                                 borrow=borrow)\n",
    "        # When storing data on the GPU it has to be stored as floats\n",
    "        # therefore we will store the labels as ``floatX`` as well\n",
    "        # (``shared_y`` does exactly that). But during our computations\n",
    "        # we need them as ints (we use labels as index, and if they are\n",
    "        # floats it doesn't make sense) therefore instead of returning\n",
    "        # ``shared_y`` we will have to cast it to int. This little hack\n",
    "        # lets ous get around this issue\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "    #-----------------------------------------------------------------\n",
    "    train_set_x, train_set_y = shared_dataset(train_vecs, train_tags)\n",
    "    valid_set_x, valid_set_y = shared_dataset(dev_vecs, dev_tags)\n",
    "    test_set_x, test_set_y = shared_dataset(test_vecs, test_tags)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "            (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "#rval=load_data(train_vecs, train_tags)\n",
    "#print rval\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    \"\"\"Multi-class Logistic Regression Class\n",
    "\n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting data\n",
    "    points onto a set of hyperplanes, the distance to which is used to\n",
    "    determine a class membership probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input of the\n",
    "                      architecture (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units, the dimension of the space in\n",
    "                     which the datapoints lie\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units, the dimension of the space in\n",
    "                      which the labels lie\n",
    "\n",
    "        \"\"\"\n",
    "        # start-snippet-1\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        # initialize the biases b as a vector of n_out 0s\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # symbolic expression for computing the matrix of class-membership\n",
    "        # probabilities\n",
    "        # Where:\n",
    "        # W is a matrix where column-k represent the separation hyperplane for\n",
    "        # class-k\n",
    "        # x is a matrix where row-j  represents input training sample-j\n",
    "        # b is a vector where element-k represent the free parameter of\n",
    "        # hyperplane-k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "\n",
    "        # symbolic description of how to compute prediction as class whose\n",
    "        # probability is maximal\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        # end-snippet-1\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        # keep track of model input\n",
    "        self.input = input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
    "        of this model under a given target distribution.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|}\n",
    "                \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
    "            \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "\n",
    "        Note: we use the mean instead of the sum so that\n",
    "              the learning rate is less dependent on the batch size\n",
    "        \"\"\"\n",
    "        # start-snippet-2\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of examples (call it n) in the minibatch\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "        # end-snippet-2\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the minibatch\n",
    "        over the total number of examples of the minibatch ; zero one\n",
    "        loss over the size of the minibatch\n",
    "\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y', y.type, 'y_pred', self.y_pred.type)\n",
    "            )\n",
    "        # check if y is of the correct datatype\n",
    "        if y.dtype.startswith('int'):\n",
    "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "            # represents a mistake in prediction\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "def sgd_optimization(learning_rate=0.13, n_epochs=1000,\n",
    "                           batch_size=600):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "    \"\"\"\n",
    "    datasets=load_data(train_vecs, train_tags, dev_vecs, dev_tags, test_vecs, test_tags)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    print train_set_x.shape[0]\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print '... building the model'\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a\n",
    "    # minibatch)\n",
    "    x = T.matrix('x')  # data, presented as rasterized images\n",
    "    y = T.ivector('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # construct the logistic regression class\n",
    "    # Each MNIST image has size 28*28\n",
    "    # MAM: We change size: n_in=space_len\n",
    "    classifier = LogisticRegression(input=x, n_in=space_len, n_out=2)\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # start-snippet-3\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-3\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print '... training the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in xrange(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "\n",
    "                    test_losses = [test_model(i)\n",
    "                                   for i in xrange(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # save the best model\n",
    "                    with open('best_model.pkl', 'w') as f:\n",
    "                        cPickle.dump(classifier, f)\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%,'\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., test_score * 100.)\n",
    "    )\n",
    "    print 'The code run for %d epochs, with %f epochs/sec' % (\n",
    "        epoch, 1. * epoch / (end_time - start_time))\n",
    "    print ('The code for file ' +\n",
    "                          'best_model.pkl' +\n",
    "                          ' ran for %.1fs' % ((end_time - start_time)))\n",
    "\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    An example of how to load a trained model and use it\n",
    "    to predict labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # load the saved model\n",
    "    classifier = cPickle.load(open('best_model.pkl'))\n",
    "\n",
    "    # compile a predictor function\n",
    "    predict_model = theano.function(\n",
    "        inputs=[classifier.input],\n",
    "        outputs=classifier.y_pred)\n",
    "\n",
    "    # We can test it on some examples from test test\n",
    "    datasets=load_data(train_vecs, train_tags, dev_vecs, dev_tags, test_vecs, test_tags)\n",
    "    #train_set_x, train_set_y = datasets[0]\n",
    "    #valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "    predicted_values = predict_model(test_set_x[:10])\n",
    "    print (\"Predicted values for the first 10 examples in test set:\")\n",
    "    print predicted_values\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_data, train_tags, dev_data, dev_tags, test_data, test_tags=get_data()\n",
    "    word_space=get_space(train_data)\n",
    "    word_space={w: word_space[w] for w in word_space if word_space[w] > 600}\n",
    "    space_len=len(word_space)\n",
    "    print(\"space_len: %d\" % space_len)\n",
    "    train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "    test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "    dev_vecs= [get_sparse_vec(data_point, word_space) for data_point in dev_data]\n",
    "    #del word_space\n",
    "    #---------------------------\n",
    "    train_vecs=np.array(train_vecs)\n",
    "    train_tags=np.array(train_tags)\n",
    "    dev_vecs=np.array(dev_vecs)\n",
    "    dev_tags=np.array(dev_tags)\n",
    "    test_vecs=np.array(test_vecs)\n",
    "    test_tags=np.array(test_tags)\n",
    "    #del train_data, train_tags, dev_data, dev_tags, test_data, test_tags\n",
    "    print('train_vecs.shape: %d, %d' % train_vecs.shape)\n",
    "    print('dev_vecs.shape: %d, %d' % dev_vecs.shape)\n",
    "    print('test_vecs.shape: %d, %d' % test_vecs.shape)\n",
    "    sgd_optimization()\n",
    "    #------------------------------------------------------\n",
    "    print('Now predicting...')\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
