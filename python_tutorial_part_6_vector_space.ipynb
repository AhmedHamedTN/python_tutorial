{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Vector Space Model, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is code to build a vector space model, with SVMs on Andrew Mass' \n",
    "# distribution of movie review sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "all_data = []  \n",
    "DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        label=line.split()[0]\n",
    "        word_list=line.lower().split()[1:]\n",
    "        all_data.append(DataDoc(label, word_list))\n",
    "        #print my_data[line_no]\n",
    "        #break\n",
    "train_data = all_data[:25000]\n",
    "test_data = all_data[25000:50000]\n",
    "print len(train_data)\n",
    "\n",
    "#train_data=train_data[:100]+train_data[12500:12600]\n",
    "#test_data=test_data[:100]+test_data[12500:12600]\n",
    "print len(train_data)\n",
    "print len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113562\n",
      "113538\n"
     ]
    }
   ],
   "source": [
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "from collections import defaultdict\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]=len(word_space)\n",
    "    return word_space\n",
    "\n",
    "word_space=get_space(train_data)\n",
    "print len(word_space)\n",
    "print word_space[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    " \n",
    "\n",
    "train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "#test_vecs= get_sparse_vectors(test_data, word_space)\n",
    "\n",
    "#print train_vecs, test_vecs[0]\n",
    "print len(train_data)\n",
    "print len(train_vecs)\n",
    "print len(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# We should usually get tags automatically based on input data file.\n",
    "# In the input data file we have, we know that the first 12500 data points are positive/1.0 and the next 12500 are\n",
    "# negative/0.0 then the next 12500 is poitive and the fourth chunk is negative.\n",
    "# So basically the train_data has 25K (with the first half positive and the second half negative)\n",
    "# and test_data with the same setup for class label. \n",
    "# The rest of the data in the file is unknown and we don't use that part.\n",
    "# We could write code to extract label automatically and we will do this based on a standardized format we will work with\n",
    "# later, for now we will hard-code the labels.\n",
    "\n",
    "from random import shuffle, randint\n",
    "\n",
    "\n",
    "train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "test_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "\n",
    "\n",
    "#train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "#test_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "# Side note: If the first token in each line were the tag, we could get tags as follows:\n",
    "# tags= [train_data[i].tag for i in range(len(train_data))]\n",
    "print train_tags[-1], train_vecs[-1][:10]\n",
    "print len(train_tags)\n",
    "print len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 113562)\n"
     ]
    }
   ],
   "source": [
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "print train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classification with scikit-learn\n",
    "# Now we have: train_tags, train_vecs, test_tags, test_vecs\n",
    "# Let's use sklearn to train an svm classifier:\n",
    "#-------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import gensim\n",
    "n_jobs = 4\n",
    "\n",
    "#train_vecs=array(train_vecs)\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "\n",
    "print type(train_tags)\n",
    "print type(train_vecs)\n",
    "clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "clf.fit(train_vecs, train_tags)\n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 5-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, train_vecs, train_tags, cv=5)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(train_tags, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(train_tags, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(train_tags, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(train_tags, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(train_tags, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X= train_vecs\n",
    "y=train_tags\n",
    "y=y.astype(int)\n",
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = len(train_vecs[0]) # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength \n",
    "\n",
    "\n",
    "def forward(W1, b1, W2, b2, x):\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    # softmax\n",
    "    y_hat = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return y_hat, z1, a1, z2\n",
    "\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    y_hat, _, _, _ = forward(W1, b1, W2, b2, x)\n",
    "    return np.argmax(y_hat, axis=1)\n",
    "\n",
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    y_hat, _, _, _ = forward(W1, b1, W2, b2, X)\n",
    "    correct_logprobs = -np.log(y_hat[range(num_examples), y])\n",
    "    data_loss = np.sum(correct_logprobs)\n",
    "    return 1./num_examples * data_loss\n",
    "\n",
    "\n",
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(nn_hdim, num_passes=2000, print_loss=False):\n",
    "    \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "        # feedforward\n",
    "        y_hat, z1, a1, z2 = forward(W1, b1, W2, b2, X)\n",
    "        \n",
    "        # Backpropagation\n",
    "        delta3 = y_hat\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        #print [range(num_examples), y]\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "          print \"Loss after iteration %i: %f\" %(i, calculate_loss(model))\n",
    "          #print y_hat[:2]\n",
    "            \n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model(3, print_loss=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
