{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Vector Space Model, with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is code to build a vector space model, with SVMs on Andrew Mass' \n",
    "# distribution of movie review sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "all_data = []  \n",
    "DataDoc= namedtuple('DataDoc', 'tag words')\n",
    "with open('/Users/mam/CORE/RESEARCH/DEEPLEARNING/Doc2Vec/data/aclImdb/alldata-id.txt') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        label=line.split()[0]\n",
    "        word_list=line.lower().split()[1:]\n",
    "        all_data.append(DataDoc(label, word_list))\n",
    "        #print my_data[line_no]\n",
    "        #break\n",
    "train_data = all_data[:25000]\n",
    "test_data = all_data[25000:50000]\n",
    "print len(train_data)\n",
    "\n",
    "train_data=train_data[:100]+train_data[12500:12600]\n",
    "test_data=test_data[:100]+test_data[12500:12600]\n",
    "print len(train_data)\n",
    "print len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7142\n",
      "6994\n"
     ]
    }
   ],
   "source": [
    "# Let's get a dictionary of all the words in training data\n",
    "# These will be our bag-of-words features\n",
    "# We won't need this function, since we will use gensim's built-in method \"Dictionary\" from the corpus module\n",
    "# --> corpora.Dictionary, but we provide this so that you are clear on one way of how to do this.\n",
    "from collections import defaultdict\n",
    "def get_space(train_data):\n",
    "    \"\"\"\n",
    "    input is a list of namedtuples\n",
    "    get a dict of word space\n",
    "    key=word\n",
    "    value=len of the dict at that point \n",
    "    (that will be the index of the word and it is unique since the dict grows as we loop)\n",
    "    \"\"\"\n",
    "    word_space=defaultdict(int)\n",
    "    for doc in train_data:\n",
    "        for w in doc.words:\n",
    "            # indexes of words won't be in sequential order as they occur in data (can you tell why?), \n",
    "            # but that doesn't matter.\n",
    "            word_space[w]=len(word_space)\n",
    "    return word_space\n",
    "\n",
    "word_space=get_space(train_data)\n",
    "print len(word_space)\n",
    "print word_space[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sparse_vec(data_point, space):\n",
    "    # create empty vector\n",
    "    sparse_vec = np.zeros((len(space)))\n",
    "    for w in set(data_point.words):\n",
    "        # use exception handling such that this function can also be used to vectorize \n",
    "        # data with words not in train (i.e., test and dev data)\n",
    "        try:\n",
    "            sparse_vec[space[w]]=1\n",
    "        except:\n",
    "            continue\n",
    "    return sparse_vec\n",
    "\n",
    " \n",
    "\n",
    "train_vecs= [get_sparse_vec(data_point, word_space) for data_point in train_data]\n",
    "test_vecs= [get_sparse_vec(data_point, word_space) for data_point in test_data]\n",
    "#test_vecs= get_sparse_vectors(test_data, word_space)\n",
    "\n",
    "#print train_vecs, test_vecs[0]\n",
    "print len(train_data[12500:12600])\n",
    "print len(train_vecs)\n",
    "print len(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# We should usually get tags automatically based on input data file.\n",
    "# In the input data file we have, we know that the first 12500 data points are positive/1.0 and the next 12500 are\n",
    "# negative/0.0 then the next 12500 is poitive and the fourth chunk is negative.\n",
    "# So basically the train_data has 25K (with the first half positive and the second half negative)\n",
    "# and test_data with the same setup for class label. \n",
    "# The rest of the data in the file is unknown and we don't use that part.\n",
    "# We could write code to extract label automatically and we will do this based on a standardized format we will work with\n",
    "# later, for now we will hard-code the labels.\n",
    "\n",
    "from random import shuffle, randint\n",
    "\n",
    "\n",
    "train_tags=[ 1.0 for i in range(100)] + [ 0.0 for i in range(100)]\n",
    "test_tags=[ 1.0 for i in range(100)] + [ 0.0 for i in range(100)]\n",
    "\n",
    "\n",
    "#train_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "#test_tags=[ 1.0 for i in range(12500)] + [ 0.0 for i in range(12500)]\n",
    "# Side note: If the first token in each line were the tag, we could get tags as follows:\n",
    "# tags= [train_data[i].tag for i in range(len(train_data))]\n",
    "print train_tags[-1], train_vecs[-1][:10]\n",
    "print len(train_tags)\n",
    "print len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 7142)\n"
     ]
    }
   ],
   "source": [
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "print train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.ndarray'>\n",
      "\n",
      "Done fitting classifier on training data...\n",
      "\n",
      "================================================== \n",
      "\n",
      "Results with 5-fold cross validation:\n",
      "\n",
      "================================================== \n",
      "\n",
      "********************\n",
      "\t accuracy_score\t0.715\n",
      "********************\n",
      "precision_score\t0.765432098765\n",
      "recall_score\t0.62\n",
      "\n",
      "classification_report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.81      0.74       100\n",
      "        1.0       0.77      0.62      0.69       100\n",
      "\n",
      "avg / total       0.72      0.71      0.71       200\n",
      "\n",
      "\n",
      "confusion_matrix:\n",
      "\n",
      "[[81 19]\n",
      " [38 62]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 750M\n"
     ]
    }
   ],
   "source": [
    "# Classification with scikit-learn\n",
    "# Now we have: train_tags, train_vecs, test_tags, test_vecs\n",
    "# Let's use sklearn to train an svm classifier:\n",
    "#-------------------------------------------------\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "import gensim\n",
    "n_jobs = 2\n",
    "\n",
    "#train_vecs=array(train_vecs)\n",
    "train_vecs=np.array(train_vecs)\n",
    "train_tags=np.array(train_tags)\n",
    "\n",
    "print type(train_tags)\n",
    "print type(train_vecs)\n",
    "clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "clf.fit(train_vecs, train_tags)\n",
    "print \"\\nDone fitting classifier on training data...\\n\"\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "print \"=\"*50, \"\\n\"\n",
    "print \"Results with 5-fold cross validation:\\n\"\n",
    "print \"=\"*50, \"\\n\"\n",
    "#------------------------------------------------------------------------------------------\n",
    "predicted = cross_validation.cross_val_predict(clf, train_vecs, train_tags, cv=5)\n",
    "print \"*\"*20\n",
    "print \"\\t accuracy_score\\t\", metrics.accuracy_score(train_tags, predicted)\n",
    "print \"*\"*20\n",
    "print \"precision_score\\t\", metrics.precision_score(train_tags, predicted)\n",
    "print \"recall_score\\t\", metrics.recall_score(train_tags, predicted)\n",
    "print \"\\nclassification_report:\\n\\n\", metrics.classification_report(train_tags, predicted)\n",
    "print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(train_tags, predicted)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Below was old code we wrote for emotion detection.\n",
    "# Now deprecated!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will be cleaned further and more notes will be added.\n",
    "The purpose is to build a vector space model for multi-class text classification.\n",
    "We use scikit-learn, but build our own code to vectorize the data.\n",
    "The example is based on emotion classification, with the 6 early Paul Ekman types of emotions: Anger, Fear, Happiness, Sadness, Disgust, and Surprise. There are other types of emotions, according to other theories. But the purpose here is to show how to build a vector space model, rather than get deeper into what types of emotions there are.\n",
    "\n",
    "There are a number of things I will change in the code, including the names of some functions.\n",
    "For example, the function with the string \"OneHotVectors\" is a misnomer. A lot of the code was written and run in a couple of class sessions, to teach text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#######################\n",
    "__version__ = \"0.5\"\n",
    "__date__ = \"Nov. 30, 2015\"\n",
    "__author__ = \"Muhammad Abdul-Mageed\"\n",
    "####################################\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "n_jobs = 25\n",
    "\n",
    "def getListOfLines():\n",
    "    \"\"\"\n",
    "    Just takes a file and returns a list of its line\n",
    "    \"\"\"\n",
    "    # Change path to file\n",
    "    return codecs.open(\"PathToFile\", \"r\", \"utf-8\").readlines()\n",
    "    \n",
    "def getThreeColumnFormat():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    infileObject=codecs.open(\"PathToFile\", \"r\", \"utf-8\")\n",
    "    listOfLines= infileObject.readlines() \n",
    "    dataTuples=[(line.split(\"\\t\")[1], line.split(\"\\t\")[2].lower()) for line in listOfLines if line.split(\"\\t\")[1] !=\"NO-EMOTION\"]\n",
    "    return dataTuples\n",
    "#####################################\n",
    "\n",
    "def tagInSecondHalf(tag, tweet):\n",
    "    \"\"\"\n",
    "    Conditioning position of tag in tweet.\n",
    "    P.S. Won't consider a tag like #happyday.\n",
    "    \"\"\"\n",
    "    tags= [\"#happy\", \"#sad\", \"#disgusted\", \"#fearful\" , \"#surprised\", \"#angry\"] #\"#scared\"\n",
    "    tweet=tweet.split()\n",
    "    if tag not in tweet:\n",
    "        return False\n",
    "    midPoint=(len(tweet)/2)\n",
    "    tagIndex=tweet.index(tag)\n",
    "    if tagIndex > midPoint:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def tagInLastThird(tag, tweet):\n",
    "    \"\"\"\n",
    "    Conditioning position of tag in tweet.\n",
    "    P.S. Won't consider a tag like #happyday.\n",
    "    \"\"\"\n",
    "    tweet=tweet.split()\n",
    "    if tag not in tweet:\n",
    "        return False\n",
    "    thirdPoint=(len(tweet)/4)\n",
    "    tagIndex=tweet.index(tag)\n",
    "    if tagIndex > thirdPoint*3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def pure(tag, tweet):\n",
    "    tagList= [\"#happy\", \"#sad\", \"#disgusted\", \"#fearful\" , \"#surprised\", \"#angry\", \"#scared\"]\n",
    "    tagList.remove(tag)\n",
    "    for t in tagList:\n",
    "        if t in tweet: \n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def removeSeed(seed, tweet):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(seed)==str:\n",
    "        tweet= re.sub(seed, \" \", tweet)\n",
    "    elif type(seed)==list:\n",
    "        for t in seed:\n",
    "            tweet= re.sub(t, \" \", tweet)\n",
    "    else:\n",
    "        print type(seed)\n",
    "        print \"arg1/Tag must be a string or list, you provided \", type(tag), \".\"\n",
    "        exit()\n",
    "    # clean\n",
    "    tweet=re.sub(\"\\s+\", \" \", tweet)\n",
    "    #tweet=tweet.trim()\n",
    "    tweet=tweet.rstrip()\n",
    "    tweet=tweet.lstrip()\n",
    "    return tweet\n",
    "\n",
    "def clean(tweet):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tweet= re.sub(\".\", \" \", tweet)\n",
    "    return tweet\n",
    "\n",
    "def longTweet(tweet):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if len(tweet.split()) > 10:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "#----------------------------------------------\n",
    "def getDataDict(emotionLines):\n",
    "    shuffle(emotionLines)\n",
    "    #emotionLines=emotionLines[:10000]\n",
    "    tagLexicon= [\"happy\", \"sad\", \"disgusted\", \"fearful\" , \"surprised\", \"angry\", \"scared\"] #\"#scared\"\n",
    "    tagDict= {\"happy\": \"HAPPINESS\", \"sad\": \"SADNESS\", \"disgusted\": \"DISGUST\", \"fearful\": \"FEAR\" , \"surprised\": \"SURPRISE\", \"angry\": \"ANGER\", \"scared\": \"FEAR\"} #\"#scared\"\n",
    "    myData={}\n",
    "    for cat in tagLexicon:\n",
    "        tag=\"#\"+cat\n",
    "        myData[tagDict[cat]]=[tweet for tweet in emotionLines if tag in tweet.split() and pure(tag, tweet)\n",
    "                 and tagInSecondHalf(tag, tweet)  and len(tweet.split()) > 4\n",
    "                 and removeSeed(tag, tweet) and clean(tweet) and longTweet(tweet)]\n",
    "    return myData\n",
    "\n",
    "def getThreeColumnDataDict(emotionLines):\n",
    "    shuffle(emotionLines)\n",
    "    #emotionLines=emotionLines[:10000]\n",
    "    classes= [\"HAPPINESS\", \"SADNESS\", \"DISGUST\", \"FEAR\" , \"SURPRISE\", \"ANGER\"]\n",
    "    myData={pair[0]: [] for pair in emotionLines}\n",
    "    for cat in classes:\n",
    "        for pair in emotionLines:\n",
    "            if pair[0]==cat:\n",
    "                myData[pair[0]].append(pair[1])\n",
    "    return myData\n",
    "\n",
    "def getDataStats(myData):\n",
    "    # Print some stats:\n",
    "    ##########################\n",
    "    majorClass=max([len(myData[k]) for k in myData])\n",
    "    totalCount=sum([len(myData[k]) for k in myData])\n",
    "    print \"Majority class count: \", majorClass\n",
    "    print \"Total data point count: \", totalCount\n",
    "    print \"Majority class % in train data: \", round((majorClass/float(totalCount))*100, 2), \"%\"\n",
    "    print \"*\"*50, \"\\n\"\n",
    "\n",
    "def getLabeledDataTuples(myData):\n",
    "    # At this point \"myData\" is a dict, with each emotion class as a key, and related tweet lines as a list of lines\n",
    "    ###############################################################\n",
    "    # The below gets me tweet body only (and filters out rest of each tweet line [e.g., tweetId.])\n",
    "    # newData will be a list of tuples, each tuple has 0 as an emotion class and 1 as the string/unicode of the tweet body\n",
    "    dataTuples=[(k, \"\".join(myData[k][i]).split(\"\\t\")[-1]) for k in myData for i in range(len(myData[k]))]\n",
    "    #shuffle(dataTuples)\n",
    "    #######################################################################\n",
    "    # See it: \n",
    "    #print \"The type of newData[0][0] is a: \", type(newData[0][0]), newData[0][0] # --> newData[0] is a string\n",
    "    #print \"The type of newData[0][1] is a: \", type(newData[0][1]), newData[0][1] # --> newData[1] is a unicode of tweet body\n",
    "    #######################################################################\n",
    "    return dataTuples\n",
    "    \n",
    "def getFeatures(dataPoint):\n",
    "    features=defaultdict()\n",
    "    # label is class name, of course, and feats is just a list of words in this case.\n",
    "    label, feats=dataPoint[0], dataPoint[1].split()\n",
    "    # I could also add some code to remove the seeds from the feature dict instead of the heavy computation in\n",
    "    # the tweet cleaning in removeSeed\n",
    "    ###########################################\n",
    "    # Beautify the below, building \"has(word): True/False\" dict\n",
    "    for i in feats:\n",
    "        features[i]=i\n",
    "    if \"#fearful\" in features:\n",
    "        del features[\"#fearful\"]\n",
    "    if \"#scared\" in features:\n",
    "        del features[\"#scared\"]\n",
    "    return features, label\n",
    "\n",
    "#featuresets=[getFeatures(i) for i in newData]\n",
    "\n",
    "def getLabelsAndVectors(dataTuples):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        dataTuples is a list of tuples\n",
    "        Each tuple in the list has\n",
    "                   0=label\n",
    "                   1= tweet body as unicode/string\n",
    "    Returns an array of labels and another array for words \n",
    "    \"\"\"\n",
    "    labels=[]\n",
    "    vectors=[]\n",
    "    ids=[]\n",
    "    c=0\n",
    "    for dataPoint in dataTuples:\n",
    "        ids.append(c)\n",
    "        c+=1\n",
    "        label, vector=dataPoint[0], dataPoint[1].split()\n",
    "        labels.append(label)\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return ids, labels, vectors\n",
    "\n",
    "def getSpace(vectors):\n",
    "    # get the dictionary of all words in train; we call it the space as it is the space of features for bag of words\n",
    "    space={}\n",
    "    for dataPoint in vectors:\n",
    "        words=dataPoint\n",
    "        for w in words:\n",
    "            if w not in space:\n",
    "                space[w]=len(space)\n",
    "    return space\n",
    "\n",
    "def augmentSpace(space, featuresList):\n",
    "    \"\"\"\n",
    "    Adds a list of features to the bag-of-words dictionary, we named \"space\".\n",
    "    \"\"\"\n",
    "    for f in featuresList:\n",
    "        if f not in space:\n",
    "            space[f]=len(space) \n",
    "    return space\n",
    "\n",
    "def getReducedSpace(vectors, space):\n",
    "    # get the dictionary of all words in train; we call it the space as it is the space of features for bag of words\n",
    "    reducedSpace=defaultdict(int)\n",
    "    for dataPoint in vectors:\n",
    "        words=dataPoint\n",
    "        for w in words:\n",
    "            reducedSpace[w]+=1\n",
    "    for w in space:\n",
    "        # could parameterize with the threshold, instead of the following\n",
    "        if reducedSpace[w] < 3:\n",
    "            del reducedSpace[w]\n",
    "    reducedSpace={w: reducedSpace[w] for w in reducedSpace}\n",
    "    return reducedSpace\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "def getOneHotVectors(ids, labels, vectors, space):\n",
    "    oneHotVectors={}\n",
    "    triples=zip(ids, labels, vectors)\n",
    "    vec = np.zeros((len(space)))\n",
    "    #for dataPoint in vectors:\n",
    "    for triple in triples:\n",
    "        idd, label, dataPoint= triple[0], triple[1], triple[2]\n",
    "        #for t in xrange(len(space)):\n",
    "        # populate a one-dimensional array of zeros of shape/length= len(space)\n",
    "        vec=np.zeros((len(space))) # ; second argument is domensionality of the array, which is 1\n",
    "        for w in dataPoint:\n",
    "            try:\n",
    "                vec[space[w]]=1\n",
    "            except:\n",
    "                continue\n",
    "        # add emotion lexicon features\n",
    "        vec=addEmotionLexiconFeatures(vec, dataPoint, space)\n",
    "        oneHotVectors[idd]=(vec, array(label))\n",
    "    return oneHotVectors\n",
    "\n",
    "def getOneHotVectorsAndLabels(oneHotVectorsDict):\n",
    "    vectors= array([oneHotVectorsDict[k][0] for k in oneHotVectorsDict])\n",
    "    labels= array([oneHotVectorsDict[k][1] for k in oneHotVectorsDict])\n",
    "    print \"labels.shape\", labels.shape \n",
    "    print \"vectors.shape\", vectors.shape \n",
    "    return vectors, labels\n",
    "###############################\n",
    "# try:\n",
    "#     vectors.shape[0]\n",
    "# except:\n",
    "#     vectors=zeros(len(vectors))\n",
    "\n",
    "# Do grid search\n",
    "#######################################\n",
    "def SVM_gridSearch(trainVectors, trainLabels, kernel):\n",
    "    C_range = 10.0 ** arange(-2, 2)\n",
    "    gamma_range = 10.0 ** arange(-2, 2)\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "    cv = StratifiedKFold(y=trainLabels, n_folds=2)\n",
    "    grid = GridSearchCV(SVC(kernel=kernel), param_grid=param_grid, cv=cv, n_jobs=n_jobs) #GridSearchCV(SVC(kernel=kernel, class_weight='auto')\n",
    "    grid.fit(trainVectors, trainLabels)\n",
    "    ##################################\n",
    "    ## Estimated best parameters\n",
    "    C = grid.best_estimator_.C\n",
    "    gamma = grid.best_estimator_.gamma\n",
    "    ##################################\n",
    "    return C, gamma\n",
    "#######################################\n",
    "\n",
    "def getCAndGamma(trainVectors, trainLabels, kernel = 'rbf'):\n",
    "    C, gamma = SVM_gridSearch(trainVectors, trainLabels, kernel)\n",
    "    print C\n",
    "    print gamma\n",
    "    return C, gamma\n",
    "\n",
    "def isRetweet(tweet):\n",
    "    if tweet.lower().split()[0] ==\"re\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "emotionFeatures=[\"hasAngerWord\", \"hasDisgustWord\", \"hasFearWord\", \"hasHappinessWord\", \"hasSadnessWord\", \"hasSurpriseWord\"]\n",
    "\n",
    "def main():\n",
    "    #######################################\n",
    "    # Saima Aman emotion blog data\n",
    "    dataTuples=getThreeColumnFormat()\n",
    "    print \"Length of saimaDataTuples is: \",  len(dataTuples)\n",
    "    #shuffle(dataTuples)\n",
    "    print \"saimaDataTuples\", dataTuples[0]\n",
    "    trainTuples=dataTuples#[:1000]\n",
    "    #testTuples=saimaDataTuples[1000:]\n",
    "\n",
    "#     #######################################\n",
    "    myData=getThreeColumnDataDict(dataTuples)\n",
    "    totalCount=sum([len(myData[k]) for k in myData])\n",
    "    print totalCount\n",
    "#     del trainLines\n",
    "#     print\"*\"*50\n",
    "    getDataStats(myData)\n",
    "#     dataTuples=getLabeledDataTuples(myData)\n",
    "#     ####################################\n",
    "#     # Add first 1000 Saima tuples\n",
    "#     #dataTuples=dataTuples+saimaDataTuples[:1000]\n",
    "#     print dataTuples[0]\n",
    "#     del myData\n",
    "    ids, labels, vectors= getLabelsAndVectors(trainTuples)\n",
    "    space=getSpace(vectors)\n",
    "    print \"Total # of features in your space is: \", len(space)\n",
    "    # augment space with emotion features...\n",
    "    space= augmentSpace(space, emotionFeatures)\n",
    "    #reducedSpace=getReducedSpace(vectors, space)\n",
    "    print \"Total # of features in your augmented space is: \", len(space)\n",
    "    #print \"Total # of features in your reducedSpace is: \", len(reducedSpace)\n",
    "    oneHotVectors=getOneHotVectors(ids, labels, vectors, space)\n",
    "    vectors, labels=getOneHotVectorsAndLabels(oneHotVectors)\n",
    "    del oneHotVectors\n",
    "    trainVectors = vectors\n",
    "    trainLabels = labels\n",
    "    del vectors\n",
    "    del labels\n",
    "    #C, gamma = getCAndGamma(trainVectors, trainLabels, kernel = 'rbf')\n",
    "    # Train classifier\n",
    "    #clf = OneVsOneClassifier(SVC(C=C, kernel=kernel, class_weight='auto', gamma=gamma, verbose= True, probability=True))\n",
    "    clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "    clf.fit(trainVectors, trainLabels)\n",
    "    print \"\\nDone fitting classifier on training data...\\n\"\n",
    "    #del trainVectors\n",
    "    #del trainLabels\n",
    "#     dataTuples=getSAIMAThreeColumnFormat()\n",
    "#     print \"Length of dataTuples is: \",  len(dataTuples)\n",
    "#     shuffle(dataTuples)\n",
    "#     print \"saimaDataTuples\", dataTuples[0]\n",
    "#     ids, labels, vectors= getLabelsAndVectors(testTuples)\n",
    "#     oneHotVectors=getOneHotVectors(ids, labels, vectors, space)\n",
    "#     vectors, labels=getOneHotVectorsAndLabels(oneHotVectors)\n",
    "#     del oneHotVectors\n",
    "#     testVectors = vectors\n",
    "#     testLabels = labels\n",
    "#     predicted_testLabels = clf.predict(testVectors)\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    print \"Results with 5-fold cross validation:\\n\"\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    predicted = cross_validation.cross_val_predict(clf, trainVectors, trainLabels, cv=5)\n",
    "    print \"*\"*20\n",
    "    print \"\\t accuracy_score\\t\", metrics.accuracy_score(trainLabels, predicted)\n",
    "    print \"*\"*20\n",
    "    print \"precision_score\\t\", metrics.precision_score(trainLabels, predicted)\n",
    "    print \"recall_score\\t\", metrics.recall_score(trainLabels, predicted)\n",
    "    print \"\\nclassification_report:\\n\\n\", metrics.classification_report(trainLabels, predicted)\n",
    "    print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(trainLabels, predicted)\n",
    "    \n",
    "    #\"------------------------------------------------------------------------------------------\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    print \"Results with 10-fold cross validation:\\n\"\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    predicted = cross_validation.cross_val_predict(clf, trainVectors, trainLabels, cv=10)\n",
    "    print \"*\"*20\n",
    "    print \"\\t accuracy_score\\t\", metrics.accuracy_score(trainLabels, predicted)\n",
    "    print \"*\"*20\n",
    "    print \"precision_score\\t\", metrics.precision_score(trainLabels, predicted)\n",
    "    print \"recall_score\\t\", metrics.recall_score(trainLabels, predicted)\n",
    "    print \"\\nclassification_report:\\n\\n\", metrics.classification_report(trainLabels, predicted)\n",
    "    print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(trainLabels, predicted)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Take a look at the metrics module at: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    #------------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Hello!!\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
