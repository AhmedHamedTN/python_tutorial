{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Vector Space Model, with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will be cleaned further and more notes will be added.\n",
    "The purpose is to build a vector space model for multi-class text classification.\n",
    "We use scikit-learn, but build our own code to vectorize the data.\n",
    "The example is based on emotion classification, with the 6 early Paul Ekman types of emotions: Anger, Fear, Happiness, Sadness, Disgust, and Surprise. There are other types of emotions, according to other theories. But the purpose here is to show how to build a vector space model, rather than get deeper into what types of emotions there are.\n",
    "\n",
    "There are a number of things I will change in the code, including the names of some functions.\n",
    "For example, the function with the string \"OneHotVectors\" is a misnomer. A lot of the code was written and run in a couple of class sessions, to teach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#######################\n",
    "__version__ = \"0.5\"\n",
    "__date__ = \"Nov. 30, 2015\"\n",
    "__author__ = \"Muhammad Abdul-Mageed\"\n",
    "####################################\n",
    "import argparse\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import os, re, glob\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from numpy import array, arange, zeros, hstack, argsort\n",
    "import unicodedata\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "n_jobs = 25\n",
    "\n",
    "def getListOfLines():\n",
    "    \"\"\"\n",
    "    Just takes a file and returns a list of its line\n",
    "    \"\"\"\n",
    "    # Change path to file\n",
    "    return codecs.open(\"PathToFile\", \"r\", \"utf-8\").readlines()\n",
    "    \n",
    "def getThreeColumnFormat():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    infileObject=codecs.open(\"PathToFile\", \"r\", \"utf-8\")\n",
    "    listOfLines= infileObject.readlines() \n",
    "    dataTuples=[(line.split(\"\\t\")[1], line.split(\"\\t\")[2].lower()) for line in listOfLines if line.split(\"\\t\")[1] !=\"NO-EMOTION\"]\n",
    "    return dataTuples\n",
    "#####################################\n",
    "\n",
    "def tagInSecondHalf(tag, tweet):\n",
    "    \"\"\"\n",
    "    Conditioning position of tag in tweet.\n",
    "    P.S. Won't consider a tag like #happyday.\n",
    "    \"\"\"\n",
    "    tags= [\"#happy\", \"#sad\", \"#disgusted\", \"#fearful\" , \"#surprised\", \"#angry\"] #\"#scared\"\n",
    "    tweet=tweet.split()\n",
    "    if tag not in tweet:\n",
    "        return False\n",
    "    midPoint=(len(tweet)/2)\n",
    "    tagIndex=tweet.index(tag)\n",
    "    if tagIndex > midPoint:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def tagInLastThird(tag, tweet):\n",
    "    \"\"\"\n",
    "    Conditioning position of tag in tweet.\n",
    "    P.S. Won't consider a tag like #happyday.\n",
    "    \"\"\"\n",
    "    tweet=tweet.split()\n",
    "    if tag not in tweet:\n",
    "        return False\n",
    "    thirdPoint=(len(tweet)/4)\n",
    "    tagIndex=tweet.index(tag)\n",
    "    if tagIndex > thirdPoint*3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def pure(tag, tweet):\n",
    "    tagList= [\"#happy\", \"#sad\", \"#disgusted\", \"#fearful\" , \"#surprised\", \"#angry\", \"#scared\"]\n",
    "    tagList.remove(tag)\n",
    "    for t in tagList:\n",
    "        if t in tweet: \n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def removeSeed(seed, tweet):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if type(seed)==str:\n",
    "        tweet= re.sub(seed, \" \", tweet)\n",
    "    elif type(seed)==list:\n",
    "        for t in seed:\n",
    "            tweet= re.sub(t, \" \", tweet)\n",
    "    else:\n",
    "        print type(seed)\n",
    "        print \"arg1/Tag must be a string or list, you provided \", type(tag), \".\"\n",
    "        exit()\n",
    "    # clean\n",
    "    tweet=re.sub(\"\\s+\", \" \", tweet)\n",
    "    #tweet=tweet.trim()\n",
    "    tweet=tweet.rstrip()\n",
    "    tweet=tweet.lstrip()\n",
    "    return tweet\n",
    "\n",
    "def clean(tweet):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    tweet= re.sub(\".\", \" \", tweet)\n",
    "    return tweet\n",
    "\n",
    "def longTweet(tweet):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if len(tweet.split()) > 10:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "#----------------------------------------------\n",
    "def getDataDict(emotionLines):\n",
    "    shuffle(emotionLines)\n",
    "    #emotionLines=emotionLines[:10000]\n",
    "    tagLexicon= [\"happy\", \"sad\", \"disgusted\", \"fearful\" , \"surprised\", \"angry\", \"scared\"] #\"#scared\"\n",
    "    tagDict= {\"happy\": \"HAPPINESS\", \"sad\": \"SADNESS\", \"disgusted\": \"DISGUST\", \"fearful\": \"FEAR\" , \"surprised\": \"SURPRISE\", \"angry\": \"ANGER\", \"scared\": \"FEAR\"} #\"#scared\"\n",
    "    myData={}\n",
    "    for cat in tagLexicon:\n",
    "        tag=\"#\"+cat\n",
    "        myData[tagDict[cat]]=[tweet for tweet in emotionLines if tag in tweet.split() and pure(tag, tweet)\n",
    "                 and tagInSecondHalf(tag, tweet)  and len(tweet.split()) > 4\n",
    "                 and removeSeed(tag, tweet) and clean(tweet) and longTweet(tweet)]\n",
    "    return myData\n",
    "\n",
    "def getThreeColumnDataDict(emotionLines):\n",
    "    shuffle(emotionLines)\n",
    "    #emotionLines=emotionLines[:10000]\n",
    "    classes= [\"HAPPINESS\", \"SADNESS\", \"DISGUST\", \"FEAR\" , \"SURPRISE\", \"ANGER\"]\n",
    "    myData={pair[0]: [] for pair in emotionLines}\n",
    "    for cat in classes:\n",
    "        for pair in emotionLines:\n",
    "            if pair[0]==cat:\n",
    "                myData[pair[0]].append(pair[1])\n",
    "    return myData\n",
    "\n",
    "def getDataStats(myData):\n",
    "    # Print some stats:\n",
    "    ##########################\n",
    "    majorClass=max([len(myData[k]) for k in myData])\n",
    "    totalCount=sum([len(myData[k]) for k in myData])\n",
    "    print \"Majority class count: \", majorClass\n",
    "    print \"Total data point count: \", totalCount\n",
    "    print \"Majority class % in train data: \", round((majorClass/float(totalCount))*100, 2), \"%\"\n",
    "    print \"*\"*50, \"\\n\"\n",
    "\n",
    "def getLabeledDataTuples(myData):\n",
    "    # At this point \"myData\" is a dict, with each emotion class as a key, and related tweet lines as a list of lines\n",
    "    ###############################################################\n",
    "    # The below gets me tweet body only (and filters out rest of each tweet line [e.g., tweetId.])\n",
    "    # newData will be a list of tuples, each tuple has 0 as an emotion class and 1 as the string/unicode of the tweet body\n",
    "    dataTuples=[(k, \"\".join(myData[k][i]).split(\"\\t\")[-1]) for k in myData for i in range(len(myData[k]))]\n",
    "    #shuffle(dataTuples)\n",
    "    #######################################################################\n",
    "    # See it: \n",
    "    #print \"The type of newData[0][0] is a: \", type(newData[0][0]), newData[0][0] # --> newData[0] is a string\n",
    "    #print \"The type of newData[0][1] is a: \", type(newData[0][1]), newData[0][1] # --> newData[1] is a unicode of tweet body\n",
    "    #######################################################################\n",
    "    return dataTuples\n",
    "    \n",
    "def getFeatures(dataPoint):\n",
    "    features=defaultdict()\n",
    "    # label is class name, of course, and feats is just a list of words in this case.\n",
    "    label, feats=dataPoint[0], dataPoint[1].split()\n",
    "    # I could also add some code to remove the seeds from the feature dict instead of the heavy computation in\n",
    "    # the tweet cleaning in removeSeed\n",
    "    ###########################################\n",
    "    # Beautify the below, building \"has(word): True/False\" dict\n",
    "    for i in feats:\n",
    "        features[i]=i\n",
    "    if \"#fearful\" in features:\n",
    "        del features[\"#fearful\"]\n",
    "    if \"#scared\" in features:\n",
    "        del features[\"#scared\"]\n",
    "    return features, label\n",
    "\n",
    "#featuresets=[getFeatures(i) for i in newData]\n",
    "\n",
    "def getLabelsAndVectors(dataTuples):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "        dataTuples is a list of tuples\n",
    "        Each tuple in the list has\n",
    "                   0=label\n",
    "                   1= tweet body as unicode/string\n",
    "    Returns an array of labels and another array for words \n",
    "    \"\"\"\n",
    "    labels=[]\n",
    "    vectors=[]\n",
    "    ids=[]\n",
    "    c=0\n",
    "    for dataPoint in dataTuples:\n",
    "        ids.append(c)\n",
    "        c+=1\n",
    "        label, vector=dataPoint[0], dataPoint[1].split()\n",
    "        labels.append(label)\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return ids, labels, vectors\n",
    "\n",
    "def getSpace(vectors):\n",
    "    # get the dictionary of all words in train; we call it the space as it is the space of features for bag of words\n",
    "    space={}\n",
    "    for dataPoint in vectors:\n",
    "        words=dataPoint\n",
    "        for w in words:\n",
    "            if w not in space:\n",
    "                space[w]=len(space)\n",
    "    return space\n",
    "\n",
    "def augmentSpace(space, featuresList):\n",
    "    \"\"\"\n",
    "    Adds a list of features to the bag-of-words dictionary, we named \"space\".\n",
    "    \"\"\"\n",
    "    for f in featuresList:\n",
    "        if f not in space:\n",
    "            space[f]=len(space) \n",
    "    return space\n",
    "\n",
    "def getReducedSpace(vectors, space):\n",
    "    # get the dictionary of all words in train; we call it the space as it is the space of features for bag of words\n",
    "    reducedSpace=defaultdict(int)\n",
    "    for dataPoint in vectors:\n",
    "        words=dataPoint\n",
    "        for w in words:\n",
    "            reducedSpace[w]+=1\n",
    "    for w in space:\n",
    "        # could parameterize with the threshold, instead of the following\n",
    "        if reducedSpace[w] < 3:\n",
    "            del reducedSpace[w]\n",
    "    reducedSpace={w: reducedSpace[w] for w in reducedSpace}\n",
    "    return reducedSpace\n",
    "\n",
    "\n",
    "#-------------------------------------------------\n",
    "def getOneHotVectors(ids, labels, vectors, space):\n",
    "    oneHotVectors={}\n",
    "    triples=zip(ids, labels, vectors)\n",
    "    vec = np.zeros((len(space)))\n",
    "    #for dataPoint in vectors:\n",
    "    for triple in triples:\n",
    "        idd, label, dataPoint= triple[0], triple[1], triple[2]\n",
    "        #for t in xrange(len(space)):\n",
    "        # populate a one-dimensional array of zeros of shape/length= len(space)\n",
    "        vec=np.zeros((len(space))) # ; second argument is domensionality of the array, which is 1\n",
    "        for w in dataPoint:\n",
    "            try:\n",
    "                vec[space[w]]=1\n",
    "            except:\n",
    "                continue\n",
    "        # add emotion lexicon features\n",
    "        vec=addEmotionLexiconFeatures(vec, dataPoint, space)\n",
    "        oneHotVectors[idd]=(vec, array(label))\n",
    "    return oneHotVectors\n",
    "\n",
    "def getOneHotVectorsAndLabels(oneHotVectorsDict):\n",
    "    vectors= array([oneHotVectorsDict[k][0] for k in oneHotVectorsDict])\n",
    "    labels= array([oneHotVectorsDict[k][1] for k in oneHotVectorsDict])\n",
    "    print \"labels.shape\", labels.shape \n",
    "    print \"vectors.shape\", vectors.shape \n",
    "    return vectors, labels\n",
    "###############################\n",
    "# try:\n",
    "#     vectors.shape[0]\n",
    "# except:\n",
    "#     vectors=zeros(len(vectors))\n",
    "\n",
    "# Do grid search\n",
    "#######################################\n",
    "def SVM_gridSearch(trainVectors, trainLabels, kernel):\n",
    "    C_range = 10.0 ** arange(-2, 2)\n",
    "    gamma_range = 10.0 ** arange(-2, 2)\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "    cv = StratifiedKFold(y=trainLabels, n_folds=2)\n",
    "    grid = GridSearchCV(SVC(kernel=kernel), param_grid=param_grid, cv=cv, n_jobs=n_jobs) #GridSearchCV(SVC(kernel=kernel, class_weight='auto')\n",
    "    grid.fit(trainVectors, trainLabels)\n",
    "    ##################################\n",
    "    ## Estimated best parameters\n",
    "    C = grid.best_estimator_.C\n",
    "    gamma = grid.best_estimator_.gamma\n",
    "    ##################################\n",
    "    return C, gamma\n",
    "#######################################\n",
    "\n",
    "def getCAndGamma(trainVectors, trainLabels, kernel = 'rbf'):\n",
    "    C, gamma = SVM_gridSearch(trainVectors, trainLabels, kernel)\n",
    "    print C\n",
    "    print gamma\n",
    "    return C, gamma\n",
    "\n",
    "def isRetweet(tweet):\n",
    "    if tweet.lower().split()[0] ==\"re\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "emotionFeatures=[\"hasAngerWord\", \"hasDisgustWord\", \"hasFearWord\", \"hasHappinessWord\", \"hasSadnessWord\", \"hasSurpriseWord\"]\n",
    "\n",
    "def main():\n",
    "    #######################################\n",
    "    # Saima Aman emotion blog data\n",
    "    dataTuples=getThreeColumnFormat()\n",
    "    print \"Length of saimaDataTuples is: \",  len(dataTuples)\n",
    "    #shuffle(dataTuples)\n",
    "    print \"saimaDataTuples\", dataTuples[0]\n",
    "    trainTuples=dataTuples#[:1000]\n",
    "    #testTuples=saimaDataTuples[1000:]\n",
    "\n",
    "#     #######################################\n",
    "    myData=getThreeColumnDataDict(dataTuples)\n",
    "    totalCount=sum([len(myData[k]) for k in myData])\n",
    "    print totalCount\n",
    "#     del trainLines\n",
    "#     print\"*\"*50\n",
    "    getDataStats(myData)\n",
    "#     dataTuples=getLabeledDataTuples(myData)\n",
    "#     ####################################\n",
    "#     # Add first 1000 Saima tuples\n",
    "#     #dataTuples=dataTuples+saimaDataTuples[:1000]\n",
    "#     print dataTuples[0]\n",
    "#     del myData\n",
    "    ids, labels, vectors= getLabelsAndVectors(trainTuples)\n",
    "    space=getSpace(vectors)\n",
    "    print \"Total # of features in your space is: \", len(space)\n",
    "    # augment space with emotion features...\n",
    "    space= augmentSpace(space, emotionFeatures)\n",
    "    #reducedSpace=getReducedSpace(vectors, space)\n",
    "    print \"Total # of features in your augmented space is: \", len(space)\n",
    "    #print \"Total # of features in your reducedSpace is: \", len(reducedSpace)\n",
    "    oneHotVectors=getOneHotVectors(ids, labels, vectors, space)\n",
    "    vectors, labels=getOneHotVectorsAndLabels(oneHotVectors)\n",
    "    del oneHotVectors\n",
    "    trainVectors = vectors\n",
    "    trainLabels = labels\n",
    "    del vectors\n",
    "    del labels\n",
    "    #C, gamma = getCAndGamma(trainVectors, trainLabels, kernel = 'rbf')\n",
    "    # Train classifier\n",
    "    #clf = OneVsOneClassifier(SVC(C=C, kernel=kernel, class_weight='auto', gamma=gamma, verbose= True, probability=True))\n",
    "    clf = OneVsRestClassifier(SVC(C=1, kernel = 'linear', gamma=1, verbose= False, probability=False))\n",
    "    clf.fit(trainVectors, trainLabels)\n",
    "    print \"\\nDone fitting classifier on training data...\\n\"\n",
    "    #del trainVectors\n",
    "    #del trainLabels\n",
    "#     dataTuples=getSAIMAThreeColumnFormat()\n",
    "#     print \"Length of dataTuples is: \",  len(dataTuples)\n",
    "#     shuffle(dataTuples)\n",
    "#     print \"saimaDataTuples\", dataTuples[0]\n",
    "#     ids, labels, vectors= getLabelsAndVectors(testTuples)\n",
    "#     oneHotVectors=getOneHotVectors(ids, labels, vectors, space)\n",
    "#     vectors, labels=getOneHotVectorsAndLabels(oneHotVectors)\n",
    "#     del oneHotVectors\n",
    "#     testVectors = vectors\n",
    "#     testLabels = labels\n",
    "#     predicted_testLabels = clf.predict(testVectors)\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    print \"Results with 5-fold cross validation:\\n\"\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    predicted = cross_validation.cross_val_predict(clf, trainVectors, trainLabels, cv=5)\n",
    "    print \"*\"*20\n",
    "    print \"\\t accuracy_score\\t\", metrics.accuracy_score(trainLabels, predicted)\n",
    "    print \"*\"*20\n",
    "    print \"precision_score\\t\", metrics.precision_score(trainLabels, predicted)\n",
    "    print \"recall_score\\t\", metrics.recall_score(trainLabels, predicted)\n",
    "    print \"\\nclassification_report:\\n\\n\", metrics.classification_report(trainLabels, predicted)\n",
    "    print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(trainLabels, predicted)\n",
    "    \n",
    "    #\"------------------------------------------------------------------------------------------\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    print \"Results with 10-fold cross validation:\\n\"\n",
    "    print \"=\"*50, \"\\n\"\n",
    "    #------------------------------------------------------------------------------------------\n",
    "    predicted = cross_validation.cross_val_predict(clf, trainVectors, trainLabels, cv=10)\n",
    "    print \"*\"*20\n",
    "    print \"\\t accuracy_score\\t\", metrics.accuracy_score(trainLabels, predicted)\n",
    "    print \"*\"*20\n",
    "    print \"precision_score\\t\", metrics.precision_score(trainLabels, predicted)\n",
    "    print \"recall_score\\t\", metrics.recall_score(trainLabels, predicted)\n",
    "    print \"\\nclassification_report:\\n\\n\", metrics.classification_report(trainLabels, predicted)\n",
    "    print \"\\nconfusion_matrix:\\n\\n\", metrics.confusion_matrix(trainLabels, predicted)\n",
    "    \n",
    "    #------------------------------------------------------------------------------------------\n",
    "    # Take a look at the metrics module at: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    #------------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print \"Hello!!\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
